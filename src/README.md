# ONNX Model Profiling with TensorRT and Nsight Tools

This project profiles the performance of Vision Transformer (ViT) models across different precision formats (FP16, MXFP8, NVFP4) using NVIDIA's TensorRT optimization and Nsight profiling tools.

## Table of Contents
- [Quick Start Guide](#quick-start-guide)
- [Overview](#overview)
- [TensorRT Deep Dive](#tensorrt-deep-dive)
- [What is a TensorRT Engine?](#what-is-a-tensorrt-engine)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Profiling Your Own Models](#profiling-your-own-models)
- [Results](#results)

---

## Quick Start Guide

**Complete step-by-step walkthrough for replicating this profiling setup on a Blackwell GPU.**

### Prerequisites

| Requirement | Minimum | Recommended |
|-------------|---------|-------------|
| **GPU** | NVIDIA Blackwell (RTX 5000 series, RTX PRO 6000) | RTX PRO 6000 |
| **Driver** | 550+ | 580+ |
| **OS** | Ubuntu 22.04 / 24.04 | Ubuntu 24.04 |
| **Docker** | 24.0+ | Latest |
| **Disk Space** | 50 GB | 100 GB |
| **RAM** | 32 GB | 64 GB |

### Step 1: Verify GPU and Driver

```bash
# Check GPU is detected
nvidia-smi

# Expected output should show Blackwell GPU:
# NVIDIA RTX PRO 6000 Blackwell Workstation Edition
# Driver Version: 580.xx.xx   CUDA Version: 13.x

# Verify compute capability (Blackwell = SM 12.0)
nvidia-smi --query-gpu=compute_cap --format=csv
```

### Step 2: Install Docker with NVIDIA Container Toolkit

```bash
# Install Docker (if not already installed)
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER
newgrp docker

# Install NVIDIA Container Toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# Configure Docker to use NVIDIA runtime
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# Verify Docker can access GPU
docker run --rm --gpus all nvidia/cuda:12.6.0-base-ubuntu24.04 nvidia-smi
```

### Step 3: Clone This Repository

```bash
cd ~/repos  # or your preferred directory
git clone <repository-url> profiling_blackwell
cd profiling_blackwell
```

### Step 4: Pull the Container Image

```bash
# Pull the NVIDIA PyTorch container (includes TensorRT, Nsight tools)
# This is ~15 GB, may take 10-20 minutes
docker pull nvcr.io/nvidia/pytorch:25.06-py3

# Verify the container works
docker run --rm --gpus all nvcr.io/nvidia/pytorch:25.06-py3 \
  bash -c "python3 -c 'import tensorrt; print(f\"TensorRT: {tensorrt.__version__}\")'"
```

### Step 5: Prepare Your ONNX Models

Place your ONNX models in the `models/` directory:

```bash
# Create models directory if it doesn't exist
mkdir -p models

# Copy your models (example)
cp /path/to/your/model_fp16.onnx models/
cp /path/to/your/model_nvfp4.onnx models/

# Verify models are in place
ls -lh models/
```

**Model naming convention** (update `scripts/run_profiling.sh` if different):
```
models/
â”œâ”€â”€ vit_fp16_bs_064.onnx    # FP16 baseline
â”œâ”€â”€ vit_mxfp8_bs_064.onnx   # MXFP8 (optional - requires plugin)
â””â”€â”€ vit_nvfp4_bs_064.onnx   # NVFP4 quantized
```

### Step 6: Configure the Profiling Script

Edit `scripts/run_profiling.sh` to match your models:

```bash
# Open the script
nano scripts/run_profiling.sh

# Find and update the MODELS array (around line 50):
declare -a MODELS=(
    "your_model_fp16.onnx:fp16"
    "your_model_nvfp4.onnx:nvfp4"
)

# Optionally adjust parameters:
WARMUP=50           # Warmup iterations
ITERATIONS=100      # Benchmark iterations
CONTAINER_IMAGE="nvcr.io/nvidia/pytorch:25.06-py3"
```

### Step 7: Run the Profiling

```bash
# Make scripts executable
chmod +x scripts/*.sh

# Option A: Quick benchmark only (fastest, ~5 min)
./scripts/run_profiling.sh --benchmark

# Option B: Full profiling with Nsight Systems (~15 min)
./scripts/run_profiling.sh --nsys

# Option C: Complete profiling (nsys + ncu + benchmark, ~30 min)
./scripts/run_profiling.sh
```

**Expected output:**
```
[2025-12-04 10:30:00] ================================================================================
[2025-12-04 10:30:00] TENSORRT PROFILING - FP16 / NVFP4
[2025-12-04 10:30:00] ================================================================================
[2025-12-04 10:30:00] >>> Checking environment...
[2025-12-04 10:30:00] GPU: NVIDIA RTX PRO 6000 Blackwell Workstation Edition
[2025-12-04 10:30:00] Docker: 29.x.x
[2025-12-04 10:30:00] Container: nvcr.io/nvidia/pytorch:25.06-py3
...
[2025-12-04 10:35:00] >>> Benchmarking: fp16
[2025-12-04 10:35:05]     Mean Latency:   10.4 ms
[2025-12-04 10:35:05]     Throughput:     95.8 qps
...
```

### Step 8: View Results

```bash
# View the summary report
cat results/runs/*/REPORT.txt

# List all generated files
find results/runs -type f

# Results structure:
results/runs/YYYYMMDD_HHMMSS/
â”œâ”€â”€ REPORT.txt              # Summary comparison
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ fp16.json           # FP16 metrics
â”‚   â””â”€â”€ nvfp4.json          # NVFP4 metrics
â”œâ”€â”€ nsight-systems/
â”‚   â”œâ”€â”€ fp16/
â”‚   â”‚   â””â”€â”€ profile.nsys-rep  # Open in Nsight Systems GUI
â”‚   â””â”€â”€ nvfp4/
â”‚       â””â”€â”€ profile.nsys-rep
â””â”€â”€ nsight-compute/
    â””â”€â”€ ...
```

### Step 9: Analyze Kernel-Level Performance

```bash
# Generate kernel summary from Nsight Systems profile
docker run --rm --gpus all \
  -v $(pwd):/workspace \
  nvcr.io/nvidia/pytorch:25.06-py3 \
  nsys stats --force-export=true \
  /workspace/results/runs/*/nsight-systems/fp16/profile.nsys-rep

# Compare FP16 vs NVFP4 kernel breakdown
docker run --rm --gpus all \
  -v $(pwd):/workspace \
  nvcr.io/nvidia/pytorch:25.06-py3 \
  bash -c "
    echo '=== FP16 Top Kernels ===' && \
    nsys stats --force-export=true /workspace/results/runs/*/nsight-systems/fp16/profile.nsys-rep 2>&1 | \
    grep -A 20 'CUDA GPU Kernel Summary' && \
    echo '' && \
    echo '=== NVFP4 Top Kernels ===' && \
    nsys stats --force-export=true /workspace/results/runs/*/nsight-systems/nvfp4/profile.nsys-rep 2>&1 | \
    grep -A 20 'CUDA GPU Kernel Summary'
  "
```

### Step 10: View in Nsight Systems GUI (Optional)

To visualize the timeline on a machine with a display:

```bash
# Option A: Copy .nsys-rep files to a machine with Nsight Systems GUI
scp results/runs/*/nsight-systems/*/*.nsys-rep user@workstation:/path/to/view/

# Option B: Install Nsight Systems locally
# Download from: https://developer.nvidia.com/nsight-systems
# Then open: nsys-ui results/runs/*/nsight-systems/fp16/profile.nsys-rep
```

---

### Troubleshooting

<details>
<summary><b>Docker: "permission denied" error</b></summary>

```bash
sudo usermod -aG docker $USER
newgrp docker
# Or logout and login again
```
</details>

<details>
<summary><b>Container: "NVIDIA driver not detected"</b></summary>

```bash
# Reinstall NVIDIA Container Toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# Test
docker run --rm --gpus all nvidia/cuda:12.6.0-base-ubuntu24.04 nvidia-smi
```
</details>

<details>
<summary><b>Engine build fails for MXFP8</b></summary>

MXFP8 models require the `TRT_MXFP8DequantizeLinear` plugin which is not included in standard containers. Either:
1. Skip MXFP8 (comment out in `run_profiling.sh`)
2. Use a container with ModelOpt TRT plugins built-in
</details>

<details>
<summary><b>Nsight Compute requires --privileged</b></summary>

The profiling script already includes `--privileged` for NCU. If you still get permission errors:

```bash
# Run container with extended privileges
docker run --rm --gpus all --privileged --cap-add=SYS_ADMIN ...
```
</details>

<details>
<summary><b>Out of GPU memory</b></summary>

Reduce batch size in your ONNX model or adjust the profiling parameters:

```bash
# In run_profiling.sh, reduce iterations
WARMUP=10
ITERATIONS=50
```
</details>

---

## Overview

### Goal
Compare inference performance **before and after TensorRT optimization** across different precision formats:

| Model | Precision | Size | Description |
|-------|-----------|------|-------------|
| `vit_fp16_bs_064.onnx` | FP16 | 166 MB | Half-precision floating point |
| `vit_mxfp8_bs_064.onnx` | MXFP8 | 87 MB | Microscaling FP8 (NVIDIA format) |
| `vit_nvfp4_bs_064.onnx` | NVFP4 | 48 MB | NVIDIA 4-bit floating point |

### Tools Used
- **NVIDIA TensorRT**: Deep learning inference optimizer
- **Nsight Systems (nsys)**: System-wide GPU activity profiler
- **Nsight Compute (ncu)**: Low-level GPU kernel profiler
- **ONNX Runtime**: Cross-platform inference engine

---

## TensorRT Deep Dive

### TensorRT vs trtexec

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            NVIDIA TensorRT SDK                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  C++ API         â”‚    â”‚  Python API      â”‚    â”‚  trtexec         â”‚      â”‚
â”‚   â”‚  (libnvinfer)    â”‚    â”‚  (tensorrt pkg)  â”‚    â”‚  (CLI tool)      â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚            â”‚                       â”‚                       â”‚                 â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                    â”‚                                         â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                         â”‚   TensorRT Runtime  â”‚                              â”‚
â”‚                         â”‚   (Core Engine)     â”‚                              â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                    â”‚                                         â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                         â”‚   CUDA Kernels      â”‚                              â”‚
â”‚                         â”‚   cuDNN, cuBLAS     â”‚                              â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Component | What It Is | When to Use |
|-----------|-----------|-------------|
| **TensorRT** | The optimization SDK/library | Building production inference systems |
| **trtexec** | Command-line tool that wraps TensorRT | Quick benchmarking, profiling, testing |
| **Python API** | Python bindings for TensorRT | Integration with Python ML pipelines |
| **C++ API** | Native TensorRT interface | Maximum performance, production deployment |

### How TensorRT Accelerates Models

#### 1. Layer/Kernel Fusion

```
BEFORE (ONNX - 5 separate CUDA kernel launches):
â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ Conv â”‚ â†’ â”‚ BN   â”‚ â†’ â”‚ ReLU â”‚ â†’ â”‚ Conv â”‚ â†’ â”‚ ReLU â”‚
â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜
   â†“          â†“          â†“          â†“          â†“
 kernel    kernel     kernel     kernel     kernel
 launch    launch     launch     launch     launch
   â”‚          â”‚          â”‚          â”‚          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Memory transfers between each!

AFTER (TensorRT - 2 fused kernel launches):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv + BN + ReLU      â”‚ â†’ â”‚  Conv + ReLU       â”‚
â”‚  (fused kernel)        â”‚   â”‚  (fused kernel)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“                            â†“
     1 kernel launch              1 kernel launch
          â”‚                            â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              No intermediate memory!
```

**Why fusion helps:**
- Fewer kernel launches (each launch has ~5-10Î¼s overhead)
- No intermediate memory read/writes
- Better GPU occupancy

#### 2. Precision Optimization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Precision Options                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   FP32 (default)     FP16              INT8           FP4/FP8   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ 32 bits   â”‚     â”‚ 16 bits   â”‚    â”‚ 8 bits    â”‚  â”‚ 4 bits â”‚ â”‚
â”‚   â”‚ per value â”‚     â”‚ per value â”‚    â”‚ per value â”‚  â”‚ per valâ”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚   Speed: 1x          Speed: 2x       Speed: 4x      Speed: 8x  â”‚
â”‚   Accuracy: 100%     Accuracy: ~99%  Accuracy: ~97% Varies     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. Kernel Auto-Tuning

For each layer, TensorRT benchmarks multiple kernel implementations:

```
MatMul Operation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Implementation 1: cuBLAS GEMM          â†’ Benchmark: 0.45ms    â”‚
â”‚  Implementation 2: cuBLAS GEMM (tiled)  â†’ Benchmark: 0.38ms    â”‚
â”‚  Implementation 3: Custom fused kernel  â†’ Benchmark: 0.31ms    â”‚
â”‚  Implementation 4: Tensor Core kernel   â†’ Benchmark: 0.28ms  âœ“ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â†‘
                                          TensorRT picks fastest
```

This is why **engine build takes minutes** - it's benchmarking hundreds of kernel variants!

#### 4. Memory Optimization

```
BEFORE:                              AFTER:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1 â”‚ â†’ Tensor A (100MB)      â”‚ Layer 1 â”‚ â†’ Buffer 1 (100MB)
â”‚ Layer 2 â”‚ â†’ Tensor B (100MB)      â”‚ Layer 2 â”‚ â†’ Buffer 1 (reused!)
â”‚ Layer 3 â”‚ â†’ Tensor C (100MB)      â”‚ Layer 3 â”‚ â†’ Buffer 2 (100MB)
â”‚ Layer 4 â”‚ â†’ Tensor D (100MB)      â”‚ Layer 4 â”‚ â†’ Buffer 1 (reused!)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total: 400MB                        Total: 200MB
```

---

## What is a TensorRT Engine?

### The Analogy: Source Code vs Compiled Binary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚   SOFTWARE WORLD                        DEEP LEARNING WORLD                 â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚   â”‚   C++ Code   â”‚                      â”‚  ONNX Model  â”‚                   â”‚
â”‚   â”‚  (portable)  â”‚                      â”‚  (portable)  â”‚                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚          â”‚                                     â”‚                            â”‚
â”‚          â”‚ compile                             â”‚ TensorRT build             â”‚
â”‚          â”‚ (gcc/clang)                         â”‚ (optimization)             â”‚
â”‚          â–¼                                     â–¼                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚   â”‚    Binary    â”‚                      â”‚   Engine     â”‚                   â”‚
â”‚   â”‚   (.exe)     â”‚                      â”‚  (.engine)   â”‚                   â”‚
â”‚   â”‚ (CPU-specific)                      â”‚ (GPU-specific)                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Comparison

| Component | What It Is | Contains | Portable? |
|-----------|-----------|----------|-----------|
| **ONNX Model** | Model definition | Weights + graph structure | âœ… Yes - runs anywhere |
| **TensorRT Engine** | Compiled model | Optimized CUDA kernels + weights | âŒ No - GPU-specific |
| **TensorRT Runtime** | Execution library | Code to run engines | âœ… Yes (with matching version) |

### What's Inside an ONNX Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ONNX FILE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Graph Definition (generic):                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Input â†’ Conv2D â†’ BatchNorm â†’ ReLU â†’ Conv2D â†’ Output    â”‚   â”‚
â”‚   â”‚         (describes WHAT to compute)                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   Weights (raw):                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  conv1.weight: [64, 3, 7, 7] floats                     â”‚   â”‚
â”‚   â”‚  conv1.bias: [64] floats                                â”‚   â”‚
â”‚   â”‚  bn1.weight: [64] floats                                â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   Metadata: opset_version, ir_version, producer                 â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Size: ~166 MB (vit_fp16)
```

### What's Inside a TensorRT Engine

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TensorRT ENGINE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Compiled CUDA Kernels (GPU-specific):                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  kernel_0: fused_conv_bn_relu_h100_sm90_fp16            â”‚   â”‚
â”‚   â”‚  kernel_1: attention_flash_h100_sm90_fp16               â”‚   â”‚
â”‚   â”‚  kernel_2: gemm_tensor_core_h100_sm90_fp16              â”‚   â”‚
â”‚   â”‚  (HOW to compute, optimized for YOUR specific GPU)      â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   Optimized Weights: reformatted for tensor cores               â”‚
â”‚   Execution Plan: memory allocation, kernel launch order        â”‚
â”‚   Device Info: Built for NVIDIA H100 (SM 9.0)                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Size: ~50-300 MB (varies based on optimizations)
```

### The Engine Build Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ONNX Model  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. PARSE                                                        â”‚
â”‚     - Read ONNX graph                                            â”‚
â”‚     - Validate operations                                        â”‚
â”‚     - Map to TensorRT layers                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. OPTIMIZE                                                     â”‚
â”‚     - Layer fusion (Conv+BN+ReLU â†’ single kernel)               â”‚
â”‚     - Precision selection (FP32 â†’ FP16/INT8)                    â”‚
â”‚     - Dead code elimination                                      â”‚
â”‚     - Constant folding                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. KERNEL AUTO-TUNE (why build takes minutes!)                 â”‚
â”‚     - Try multiple kernel implementations                        â”‚
â”‚     - Benchmark each on YOUR specific GPU                        â”‚
â”‚     - Select fastest for each layer                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. SERIALIZE                                                    â”‚
â”‚     - Pack everything into .engine file                         â”‚
â”‚     - Store selected kernels + weights + execution plan         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ENGINE     â”‚ â† Ready to run, no more optimization needed
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Engines Are GPU-Specific

```
Same ONNX Model â†’ Different Engines for Different GPUs:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ONNX Model  â”‚
â”‚  (portable)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Build on H100â”‚    â”‚ Build on A100â”‚    â”‚Build on 4090 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Engine (H100)â”‚    â”‚ Engine (A100)â”‚    â”‚ Engine (4090)â”‚
â”‚  - SM 9.0    â”‚    â”‚  - SM 8.0    â”‚    â”‚  - SM 8.9    â”‚
â”‚  - 80GB HBM3 â”‚    â”‚  - 80GB HBM2eâ”‚    â”‚  - 24GB GDDR6â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                    â”‚
       â–¼                    â–¼                    â–¼
   âœ— Won't run          âœ— Won't run          âœ— Won't run
   on A100!             on H100!             on H100!
```

---

## Complete Profiling Workflow

```
                              PROFILING WORKFLOW
                              
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                              â”‚
    â”‚   1. MODEL EXPORT (done previously)                         â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚   â”‚ PyTorch â”‚ â†’  â”‚  ONNX   â”‚    â”‚ Quantizeâ”‚                â”‚
    â”‚   â”‚  Model  â”‚    â”‚ Export  â”‚ â†’  â”‚ (FP16,  â”‚                â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ MXFP8,  â”‚                â”‚
    â”‚                                 â”‚ NVFP4)  â”‚                â”‚
    â”‚                                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                â”‚
    â”‚                                      â”‚                      â”‚
    â”‚   2. ENGINE BUILD (TensorRT)         â–¼                      â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚   â”‚  vit_fp16.onnx â”€â”€â”                      â”‚               â”‚
    â”‚   â”‚  vit_mxfp8.onnx â”€â”¼â”€â†’ TensorRT Builder   â”‚               â”‚
    â”‚   â”‚  vit_nvfp4.onnx â”€â”˜   (optimize+compile) â”‚               â”‚
    â”‚   â”‚                            â”‚            â”‚               â”‚
    â”‚   â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚               â”‚
    â”‚   â”‚                   â–¼        â–¼        â–¼   â”‚               â”‚
    â”‚   â”‚              .engine  .engine  .engine  â”‚               â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
    â”‚                                                              â”‚
    â”‚   3. PROFILED INFERENCE (Nsight Systems)                    â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚   â”‚  nsys wraps inference execution         â”‚               â”‚
    â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚               â”‚
    â”‚   â”‚  â”‚ Load Engine                     â”‚    â”‚               â”‚
    â”‚   â”‚  â”‚ Warm-up (50 iterations)         â”‚    â”‚               â”‚
    â”‚   â”‚  â”‚ Benchmark (100 iterations) â—€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€ Measured    â”‚
    â”‚   â”‚  â”‚ Record all GPU activity    â—€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€ Profiled    â”‚
    â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚               â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
    â”‚                          â”‚                                   â”‚
    â”‚                          â–¼                                   â”‚
    â”‚   4. OUTPUTS                                                â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚   â”‚  profile.nsys-rep  â†’ Open in Nsight GUI â”‚               â”‚
    â”‚   â”‚  metrics.json      â†’ Latency/Throughput â”‚               â”‚
    â”‚   â”‚  COMPARISON.txt    â†’ Summary report     â”‚               â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
    â”‚                                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Nsight Systems Timeline View

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Nsight Systems Timeline                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ CPU Thread  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘                       â”‚
â”‚             â”‚       â”‚       â”‚       â”‚       â”‚                               â”‚
â”‚             â–¼       â–¼       â–¼       â–¼       â–¼                               â”‚
â”‚ CUDA API    â—       â—       â—       â—       â—  (kernel launches)           â”‚
â”‚             â”‚       â”‚       â”‚       â”‚       â”‚                               â”‚
â”‚             â–¼       â–¼       â–¼       â–¼       â–¼                               â”‚
â”‚ GPU Stream  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“  (kernel execution)        â”‚
â”‚                                                                             â”‚
â”‚ Memory      â”€â”€â–²â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â–¼â”€â”€  (HtoD, DtoH transfers)   â”‚
â”‚                                                                             â”‚
â”‚ Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶       â”‚
â”‚             0ms    2ms    4ms    6ms    8ms   10ms                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Gets Measured in Inference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Inference Breakdown                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ Input Copy  â”‚  HtoD: Host memory â†’ GPU memory (~0.1ms)      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  Kernel 1   â”‚â†’ â”‚  Kernel 2   â”‚â†’ â”‚  Kernel 3   â”‚  ...        â”‚
â”‚  â”‚  (Attention)â”‚  â”‚  (FFN)      â”‚  â”‚  (LayerNorm)â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                                    â”‚                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                    GPU Compute (~7ms for FP16+TRT)              â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ Output Copy â”‚  DtoH: GPU memory â†’ Host memory (~0.05ms)     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚                                                                 â”‚
â”‚  Total Latency = Input + Compute + Output                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
profiling/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”‚
â”œâ”€â”€ models/                             # ONNX models
â”‚   â”œâ”€â”€ vit_fp16_bs_064.onnx           # FP16 model (166 MB)
â”‚   â”œâ”€â”€ vit_mxfp8_bs_064.onnx          # MXFP8 model (87 MB)
â”‚   â””â”€â”€ vit_nvfp4_bs_064.onnx          # NVFP4 model (48 MB)
â”‚
â”œâ”€â”€ scripts/                            # Profiling scripts
â”‚   â”œâ”€â”€ run_profiling.sh               # Main profiling script (nsys + ncu + benchmark)
â”‚   â””â”€â”€ setup.sh                       # Environment setup
â”‚
â”œâ”€â”€ src/                                # Python source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ benchmark.py                   # ONNX benchmarking utilities
â”‚   â”œâ”€â”€ profiler.py                    # NCU profiling wrapper
â”‚   â”œâ”€â”€ analyzer.py                    # Result analysis
â”‚   â”œâ”€â”€ compare.py                     # Result comparison utilities
â”‚   â”œâ”€â”€ compare_all.py                 # Multi-result comparison
â”‚   â””â”€â”€ visualizer.py                  # Generate charts/plots
â”‚
â”œâ”€â”€ configs/                            # Configuration files
â”‚   â””â”€â”€ profiling_config.yaml          # Centralized profiling configuration
â”‚
â”œâ”€â”€ tools/                              # External tool wrappers
â”‚   â”œâ”€â”€ nsys_wrapper.sh                # Nsight Systems wrapper
â”‚   â”œâ”€â”€ ncu_wrapper.sh                 # Nsight Compute wrapper
â”‚   â”œâ”€â”€ verify_cuda_setup.sh           # CUDA environment verification
â”‚   â””â”€â”€ transfer_package.sh            # File transfer utility
â”‚
â”œâ”€â”€ results/                            # Profiling outputs (generated)
â”‚   â”œâ”€â”€ nsight-systems/                # Nsight Systems results (.nsys-rep)
â”‚   â”œâ”€â”€ nsight-compute/                # Nsight Compute results (.ncu-rep)
â”‚   â”œâ”€â”€ benchmark/                     # Benchmark results (JSON)
â”‚   â””â”€â”€ PROFILING_REPORT_*.txt         # Summary reports
â”‚
â”œâ”€â”€ engines/                            # TensorRT engines (generated)
â”‚   â””â”€â”€ *.engine                       # Compiled TRT engines
â”‚
â””â”€â”€ logs/                               # Execution logs (generated)
    â””â”€â”€ profiling_*.log
```

---

## Usage

### Prerequisites

- **Docker** with NVIDIA Container Toolkit
- **NVIDIA GPU** with driver installed
- **Container**: `nvcr.io/nvidia/pytorch:25.06-py3` (auto-pulled)

```bash
# Setup environment (pulls container, verifies GPU)
./scripts/setup.sh

# Container includes:
#   - TensorRT 10.11
#   - ModelOpt 0.29 (for MXFP8 support)
#   - Nsight Systems (nsys)
#   - Nsight Compute (ncu)
```

### Run Profiling

All profiling uses Docker containers - no local installation needed.

```bash
# Full profiling (Nsight Systems + Nsight Compute + Benchmark)
./scripts/run_profiling.sh

# Nsight Systems only (GPU timeline, ~10 min)
./scripts/run_profiling.sh --nsys

# Nsight Compute only (kernel metrics, ~30 min - slowest)
./scripts/run_profiling.sh --ncu

# Benchmark only (no profiling overhead, ~5 min - fastest)
./scripts/run_profiling.sh --benchmark

# Build TensorRT engines only
./scripts/run_profiling.sh --build
```

### View Results

```bash
# Text report
cat results/nsight-systems/COMPARISON_REPORT_*.txt

# Open Nsight Systems GUI (on local machine with GUI)
nsys-ui results/nsight-systems/vit_fp16_trt_*/profile.nsys-rep
```

---

## Profiling Your Own Models

### Exporting Your Model to ONNX

If you have a PyTorch model, export it to ONNX:

```python
import torch
import torch.onnx

# Load your model
model = YourModel()
model.eval()

# Create dummy input matching your model's expected input
batch_size = 64
dummy_input = torch.randn(batch_size, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "models/your_model_fp16.onnx",
    export_params=True,
    opset_version=17,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    }
)
```

### Quantizing to NVFP4

Use NVIDIA ModelOpt to quantize your ONNX model:

```python
# Inside the container or with modelopt installed
import modelopt.onnx.quantization as moq

# Quantize to NVFP4
moq.quantize(
    onnx_path="models/your_model_fp16.onnx",
    output_path="models/your_model_nvfp4.onnx",
    quantize_mode="nvfp4_awq_clip",
)
```

Or use the container:

```bash
docker run --rm --gpus all \
  -v $(pwd):/workspace \
  nvcr.io/nvidia/pytorch:25.06-py3 \
  python3 -c "
import modelopt.onnx.quantization as moq
moq.quantize(
    '/workspace/models/your_model_fp16.onnx',
    '/workspace/models/your_model_nvfp4.onnx',
    quantize_mode='nvfp4_awq_clip'
)
"
```

### Updating the Profiling Script

1. **Edit model list** in `scripts/run_profiling.sh`:

```bash
declare -a MODELS=(
    "your_model_fp16.onnx:fp16"
    "your_model_nvfp4.onnx:nvfp4"
)
```

2. **Adjust input shapes** if needed (in trtexec commands):

```bash
# For dynamic shapes, add to trtexec:
--minShapes=input:1x3x224x224 \
--optShapes=input:64x3x224x224 \
--maxShapes=input:128x3x224x224
```

### Expected Results by Model Size

| Model Size | FP16 Engine | NVFP4 Engine | Build Time |
|------------|-------------|--------------|------------|
| ~100M params | ~400 MB | ~150 MB | ~2 min |
| ~300M params | ~1.2 GB | ~400 MB | ~5 min |
| ~1B params | ~4 GB | ~1.5 GB | ~15 min |

---

## Results

### Example: FP16 CUDA vs TensorRT

| Configuration | Mean Latency | P95 Latency | Throughput |
|--------------|-------------|-------------|------------|
| FP16 + CUDA | 2793.07 ms | 3913.97 ms | 22.91 imgs/sec |
| FP16 + TensorRT | 7.44 ms | 7.48 ms | 8,607 imgs/sec |

**TensorRT Speedup: 375x** ğŸš€

### Why Such Large Speedup?

| Factor | CUDA Provider | TensorRT |
|--------|--------------|----------|
| Kernel fusion | None | âœ… Aggressive fusion |
| Precision | FP32 compute | FP16 Tensor Cores |
| Memory | Naive allocation | Optimized reuse |
| Kernels | Generic cuDNN | Auto-tuned for GPU |

---

## Known Issues

### MXFP8/NVFP4 Model Compatibility

The MXFP8 and NVFP4 models contain TensorRT-specific custom operators:
- `trt` domain operators
- `trt.plugins` domain operators

These require **native TensorRT** (not through ONNX Runtime) to execute:

```
Standard ONNX Model (FP16):     â†’ Works with ONNX Runtime âœ“
TRT-Specific Model (MXFP8):     â†’ Requires native TensorRT
```

**Solution**: Use `trtexec` directly or TensorRT Python API.

---

## References

- [TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)
- [Nsight Systems User Guide](https://docs.nvidia.com/nsight-systems/)
- [Nsight Compute Documentation](https://docs.nvidia.com/nsight-compute/)
- [ONNX Runtime TensorRT EP](https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html)

---

## License

Internal NVIDIA project for performance profiling and optimization research.
