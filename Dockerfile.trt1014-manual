# TensorRT 10.14 Container with Full Binary Support
# 
# PREREQUISITES:
# 1. Download TensorRT 10.14 tarball from NVIDIA Developer:
#    https://developer.nvidia.com/tensorrt
#    
# 2. Select: TensorRT 10.14.1 GA -> Linux -> x86_64 -> CUDA 12.x -> TAR Package
#    File: TensorRT-10.14.1.48.Linux.x86_64-gnu.cuda-12.8.tar.gz (~3GB)
#
# 3. Place the tarball in this directory (same as Dockerfile)
#
# 4. Build:
#    docker build -f Dockerfile.trt1014-manual -t tensorrt-10.14-blackwell:latest .
#
# 5. Run:
#    docker run --rm --gpus all -v $(pwd):/workspace tensorrt-10.14-blackwell:latest \
#        trtexec --onnx=/workspace/models/vit_nvfp4_bs_064.onnx --fp16 --stronglyTyped --verbose

FROM nvcr.io/nvidia/tensorrt:25.06-py3

# Copy TensorRT tarball (user must download this manually)
# Expected filename pattern: TensorRT-10.14.*.tar.gz
COPY TensorRT-10.14*.tar.gz /tmp/

# Extract and install TensorRT 10.14
RUN cd /tmp && \
    tar -xzf TensorRT-10.14*.tar.gz && \
    TRT_DIR=$(ls -d TensorRT-10.14* | head -1) && \
    \
    # Backup old TensorRT
    mv /opt/tensorrt /opt/tensorrt-10.11-backup && \
    \
    # Install new TensorRT
    mv /tmp/$TRT_DIR /opt/tensorrt && \
    \
    # Install Python bindings
    pip install /opt/tensorrt/python/tensorrt-*-cp312-*.whl && \
    \
    # Update library paths
    echo "/opt/tensorrt/lib" > /etc/ld.so.conf.d/tensorrt.conf && \
    ldconfig && \
    \
    # Cleanup
    rm -f /tmp/TensorRT-10.14*.tar.gz

# Update PATH and LD_LIBRARY_PATH
ENV PATH=/opt/tensorrt/bin:${PATH}
ENV LD_LIBRARY_PATH=/opt/tensorrt/lib:${LD_LIBRARY_PATH}

# Verify installation
RUN trtexec --help 2>&1 | head -3 && \
    python3 -c "import tensorrt as trt; print(f'TensorRT Python: OK')"

WORKDIR /workspace

